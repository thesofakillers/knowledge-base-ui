---
date: 2020-10-08 15:31:20
title: Reverse-mode Automatic Differentiation 
id: 2020-10-08t15-31-20z
---

[Autodiff](./2020-10-08t15-20-39z.md) is a way of computing derivatives of an
arbitrary function with respect to one or more of its input variables through
the use of repeated applications of the chain rule on intermediate
representations underlying the original function.

Reverse-mode autodiff addresses the $O(n)$ complexity limitation of
[forward-mode autodiff](./2020-10-08t15-33-40z.md) with respect to $n$
variables by computing the gradient of a given output variable $y_j$ with
respect to all input variables $x_k$ in one pass.

As such, the time complexity reverse-mode autodiff scales linearly with the
number of output variables $m$ it is used to compute the gradient for, i.e. it
is $O(m)$ complex in time.

To perform reverse-mode autodiff given an output variable $y_j$ of function
$\mathbf{y}(x_1, \ldots, x_n)$, we start by performing a forward pass through
the function, building a computational graph in the process like we did in
forward-mode autodiff. Here however, we do not simultaneously compute the
differentials, but instead store the intermediate representations at each node
in the graph, along with their dependencies in a data structure known as
a Wengert list, also known as the "tape".

Once the graph is built, we then work our way backwards (i.e. from the output
variable), calculating the derivative of the output with respect to the
intermediate variables $v_i$ that we computed. We refer to these derivatives as
_adjoints_ and represent them with a bar $\bar{v_i}$

To calculate the adjoint $\bar{v_i}$ of a given intermediate variable with $T$
child nodes, we of course use the chain rule

$$
\bar{v_i} = \frac{\partial y_j}{\partial v_i} = \sum_t^T{\bar{v_t}\frac{\partial
v_t}{\partial v_i}}.
$$

As mention above then, the gradient of a given output variable $y_j$ is
computed in a single pass. We have the derivatives of $y_j$ with respect to all
the input variables.

This is ideal for machine learning, where we have an immense amount of input
variables (parameters), for few outputs.

This [video](https://www.youtube.com/watch?v=wG_nF1awSSY) may be useful for
visualizing what is happening.

This
[article](https://rufflewind.com/2016-12-30/reverse-mode-automatic-differentiation)
is useful for going slightly into depth about the implementation.
