---
date: 2020-09-04 13:53:54
title: Machine Learning on Sequences without RNNs
id: 2020-09-04t13-53-54z
---

Nowadays, the more common approach for problems dealing with sequences (text,
time series data, etc.) in the Machine Learning world is to utilize some
variants of [RNNs](./2020-09-04t13-57-25z.md)

However this is not strictly necessary. In
[@agrawalMachineLearningPrecipitation2019], the authors utilize adjacent
channels in the input to pass information from adjacent channels. Essentially,
they simply add a separate dimension to the input tensor for handling time. 

An issue I can see with this is that you need to have your entire input
sequence ready for the inference of a single successive point, whereas with
RNNs you can continuously feed new input and perform inference at each new
point.

## References
