---
date: 2020-09-04 13:53:54
title: Machine Learning on Sequences without RNNs
id: 2020-09-04t13-53-54z
---

Nowadays, the more common approach for problems dealing with sequences (text,
[time series data](./2020-09-23t15-18-55z.md), etc.) in the Machine Learning
world is to utilize some variants of [RNNs](./2020-09-04t13-57-25z.md)

However this is not strictly necessary. In
[@agrawalMachineLearningPrecipitation2019], the authors utilize adjacent
channels in the input to pass information from adjacent steps for 
[time series forecasting of rain](./2020-08-30t15-46-28z.md). Essentially,
they simply add a separate dimension to the input tensor for handling time.

An issue I can see with this is that you need to have your entire input
sequence ready for the inference of a single successive point, whereas with
RNNs you can continuously feed new input and perform inference at each new
point.

## References
