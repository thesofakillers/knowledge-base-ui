---
date: 2020-10-09 13:05:08
title: Numerical Differentiation 
id: 2020-10-09t13-05-08z
---

Numerical Differentiation refers to a class of algorithms for estimating the
derivative of a function by using the values of the function across several
points.

The simplest way form of Numerical Differentiation is the method of Finite
Differences, where we make use of the [limit definition of
a derivative](https://en.wikipedia.org/wiki/Derivative#Rigorous_definition) to
approximate the gradient.

Mathematically, for a function $f:\mathbb{R}^n\mapsto\mathbb{R}$, then the
partial derivative with respect to some variable $x_i$ can be approximated as

$$
\frac{\partial f}{\partial x_i} \approx \frac{f(x + he_i) - f(x)}{h},
$$

where $e_i$ is the unit vector along the $ith$ axis and $h$ is some small
($\approx 10^{-5}$) step size.

---

Traditional, finite difference based numerical differentiation suffers from 

- **truncation error**: error that emerges given that while $h$ is meant to
  approximate 0, it still isn't 0.
- **rounding error**: floating point precision limits that are reached for an
  overly small $h$.
- **$O(n)$ evaluations for $n$ dimensional gradients**: quite computationally
  expensive for complicated gradients.
